{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1PVkFkstUEQ1UNuhRua/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabilera1/myColabNotebook/blob/main/Iris_%EB%B6%93%EA%BD%83_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B_Xgboost_Random_Forest_Classifier_RFC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-rmyBh-92pcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ed92c0-c89c-4c2f-bfd5-76b3d73256d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best RFC Params: {'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best RFC Accuracy: 0.9500000000000001\n",
            "Best XGB Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Best XGB Accuracy: 0.9583333333333334\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 데이터 로드\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 트레인/테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 랜덤 포레스트 하이퍼파라미터 그리드 설정\n",
        "rfc_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 4, 6]\n",
        "}\n",
        "\n",
        "# XGBoost 하이퍼파라미터 그리드 설정\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1]\n",
        "}\n",
        "\n",
        "# 랜덤 포레스트 모델 생성 및 그리드 서치\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "rfc_grid = GridSearchCV(rfc, rfc_params, cv=3, scoring='accuracy', verbose=1)\n",
        "rfc_grid.fit(X_train, y_train)\n",
        "\n",
        "# XGBoost 모델 생성 및 그리드 서치\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='accuracy', verbose=1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Best RFC Params:\", rfc_grid.best_params_)\n",
        "print(\"Best RFC Accuracy:\", rfc_grid.best_score_)\n",
        "print(\"Best XGB Params:\", xgb_grid.best_params_)\n",
        "print(\"Best XGB Accuracy:\", xgb_grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 랜덤 포레스트 하이퍼파라미터 그리드 설정\n",
        "rfc_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 4, 6]\n",
        "}\n",
        "\n",
        "# XGBoost 하이퍼파라미터 그리드 설정\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1]\n",
        "}\n",
        "\n",
        "\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "rfc_grid = GridSearchCV(rfc, rfc_params, cv=3, scoring='accuracy', verbose=1)\n",
        "rfc_grid.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='accuracy', verbose=1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Best RFC Params:\", rfc_grid.best_params_)\n",
        "print(\"Best RFC Accuracy:\", rfc_grid.best_score_)\n",
        "print(\"Best XGB Params:\", xgb_grid.best_params_)\n",
        "print(\"Best XGB Accuracy:\", xgb_grid.best_score_)\n"
      ],
      "metadata": {
        "id": "wNfP7Rp9fF0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# XGBoost 하이퍼파라미터 그리드 설정\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1]\n",
        "}\n",
        "\n",
        "\n",
        "XGBoost의 주요 하이퍼파라미터를 튜닝할 때 고려할 수 있는 매개변수는 다양합니다. 여기서는 n_estimators, max_depth, learning_rate, subsample 등 몇 가지 중요한 하이퍼파라미터에 대해 상세히 설명하겠습니다.\n",
        "\n",
        "1. n_estimators\n",
        "설명: 이 매개변수는 부스팅 단계의 수, 즉 약한 학습기(weak learner)의 수를 결정합니다. 부스팅 모델은 이 약한 학습기들을 순차적으로 학습시켜 점차 강한 모델을 구성합니다.\n",
        "값의 범위: 일반적으로 더 많은 수의 약한 학습기가 있을수록 모델의 성능이 좋아질 수 있지만, 너무 많이 설정하면 오버피팅(과적합)이 발생할 위험이 있습니다. 적절한 수를 선택하는 것이 중요하며, 이 예제에서는 100, 200, 300을 선택하였습니다.\n",
        "2. max_depth\n",
        "설명: 각 약한 학습기의 최대 깊이를 결정합니다. 깊이가 깊을수록 모델은 더 복잡한 패턴을 학습할 수 있지만, 오버피팅의 위험이 증가합니다.\n",
        "값의 범위: 일반적으로 3에서 10 사이의 값이 사용됩니다. 너무 깊으면 학습 데이터에 과적합할 수 있으며, 너무 얕으면 학습이 충분히 이루어지지 않을 수 있습니다. 이 예제에서는 3, 5, 7을 선택하였습니다.\n",
        "3. learning_rate\n",
        "설명: 학습 단계별로 얼마나 많은 \"학습\"이 이루어질 것인지를 결정하는 매개변수입니다. 이는 각 부스팅 단계에서 모델의 업데이트 크기를 조절합니다.\n",
        "값의 범위: 일반적으로 낮은 학습률(예: 0.01-0.1)을 선택하면 더 많은 부스팅 단계가 필요하고, 결과적으로 모델이 더 천천히 학습됩니다. 그러나 이는 때때로 더 좋은 일반화 성능을 낼 수 있습니다. 높은 학습률은 빠른 수렴을 의미하지만 오버피팅을 유발할 수 있습니다. 이 예제에서는 0.01, 0.1, 0.2를 사용하였습니다.\n",
        "4. subsample\n",
        "설명: 각 부스팅 단계에서 사용할 훈련 데이터 샘플의 비율을 설정합니다. 이를 통해 각 단계에서의 데이터의 임의성을 증가시켜 모델의 일반화 능력을 향상시킬 수 있습니다.\n",
        "값의 범위: 일반적으로 0.5에서 1 사이의 값이 사용됩니다. 1은 전체 데이터를 사용함을 의미하며, 1 미만의 값은 그만큼 샘플을 무작위로 선택하여 사용함을 의미합니다. 이 예제에서는 0.8, 0.9, 1을 선택하였습니다.\n",
        "이와 같이 하이퍼파라미터를 조절하여 XGBoost 모델의 성능을 최적화할 수 있으며, GridSearchCV를 사용하여 이들 매개변수의 최적 조합을 자동으로 찾아낼 수 있습니다.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0LAdrNIyE_RR"
      }
    }
  ]
}